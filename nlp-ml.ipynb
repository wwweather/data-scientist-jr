{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Natural Language Processing (ML)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1691690395761,"user":{"displayName":"Ольга Нелюбова","userId":"07415994700883317500"},"user_tz":-480},"id":"3MAKR73RlwGU"},"outputs":[],"source":["# Импортировать инструменты\n","import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","\n","from sklearn.linear_model import RidgeClassifier\n","from sklearn.metrics import roc_auc_score\n","\n","from scipy.sparse import csr_matrix\n","from scipy.sparse import hstack\n","\n","import re\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1572,"status":"ok","timestamp":1691690397678,"user":{"displayName":"Ольга Нелюбова","userId":"07415994700883317500"},"user_tz":-480},"id":"PXJ7JmyQlwGV","outputId":"20d0d322-4842-4c43-8872-ffe744bf4abc"},"outputs":[],"source":["comments_corpus = pd.read_csv('../toxic.csv', skipinitialspace=True)\n","\n","# Проверка данных на наличие проблемых проблем\n","#comments_corpus.isnull().values.any()\n","#comments_corpus.duplicated().values.any()\n","\n","rowsums = comments_corpus.iloc[:,2:].sum(axis=1)\n","all_zero = (rowsums==0)\n","\n","print(f\"Общее количество комментариев в сете: {len(comments_corpus)}.\")\n","print(f\"Количество комментариев категории toxic: {comments_corpus['toxic'].sum()}.\")\n","print(f\"Количество комментариев категории severe toxic: {comments_corpus['severe_toxic'].sum()}.\")\n","print(f\"Количество комментариев категории obscene: {comments_corpus['obscene'].sum()}.\")\n","print(f\"Количество комментариев категории threat: {comments_corpus['threat'].sum()}.\")\n","print(f\"Количество комментариев категории insult: {comments_corpus['insult'].sum()}.\")\n","print(f\"Количество комментариев категории identity hate: {comments_corpus['identity_hate'].sum()}.\")\n","print(f\"Общее количество корректных комментариев: {all_zero.sum()}.\")\n","\n","# Организовать новый столбец данных по нетоксичным комментариям из сета\n","cc_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n","comments_corpus['non_toxic'] = comments_corpus[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].max(axis=1)\n","cc_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'non_toxic']\n","comments_corpus = comments_corpus.replace({'non_toxic':{0:1, 1:0}})\n","\n","sorted_corpus = comments_corpus[cc_columns].sum().sort_values(ascending=False)\n","\n","# Нарисовать диаграмму\n","plt.figure(figsize = (8,9))\n","ax = sns.barplot(x = sorted_corpus.index, y = sorted_corpus.values, palette = 'colorblind', width = 0.9)#, hue = cc_columns)\n","plt.xticks(rotation=45)\n","ax.set(xlabel = 'Категории', ylabel = 'Количество комментариев')\n","ax.set_title('Диаграмма разделения комментариев по категориям')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AaovhHgklwGW"},"outputs":[],"source":["pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_colwidth', None)\n","\n","#!pip install nltk\n","#nltk.download('all')\n","\n","# Оставить токены, содержащие символы английского алфавита (НЕ ТОЛЬКО)\n","\n","def cleaning(data):\n","    # Привести текст к нижнему регистру\n","    data = data.lower()\n","    # Убрать знаки препинания, цифры\n","    etc = r'[\\s+|[,.\\\"!?@|_:;，\\(\\)\\'/...=\\►╟\\─\\-\\•✆\\[\\]\\&#~`→+*%><\\\\\\{\\}\\|]]'  # Возможно несовпадение при разных условиях\n","    data = re.sub(etc,'', data)\n","    data = re.sub(r'(\\d+\\s\\d+)|(\\d+)','', data)\n","    return data\n","\n","comment_tokenized = []\n","lemmatizer = WordNetLemmatizer()\n","\n","# Построчная обработка сета\n","for text in comments_corpus['comment_text']:\n","    text = cleaning(text)\n","    # Токенизировать текст\n","    text = word_tokenize(text)\n","    # Убрать стоп-слова\n","    text = [lemmatizer.lemmatize(word) for word in text if not word in stopwords.words('english') and word.isalpha()]\n","    comment_tokenized.append(text)\n","\n","comments_corpus['comment_tokenized'] = comment_tokenized\n","\n","# Поиск комментария по индексу\n","search_id = '000103f0d9cfb60f'\n","comment_id = comments_corpus[comments_corpus['id'] == search_id]\n","print(f\"Количество токенов по индексу {search_id}: {len(comment_id.iloc[0]['comment_tokenized'])}.\")\n","# Можно продолжить как-нибудь"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"aborted","timestamp":1691690511624,"user":{"displayName":"Ольга Нелюбова","userId":"07415994700883317500"},"user_tz":-480},"id":"nm48y6c-lwGW"},"outputs":[],"source":["comments_corpus['Token_comment'] = [' '.join(item) for item in comments_corpus['comment_tokenized']]\n","X = comments_corpus['Token_comment']\n","Y = comments_corpus.drop(['id','Token_comment', 'comment_text', 'comment_tokenized'], axis=1)\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=21, shuffle=True)\n","\n","vectorizer = TfidfVectorizer(max_features=3000)\n","vectorizer.fit(X_train)\n","vectorizer.fit(X_test)\n","X_train_transform = vectorizer.transform(X_train)\n","X_test_transform = vectorizer.transform(X_test)\n","\n","print(f\"Размерность тренировочной выборки после преобразования текста: {X_train_transform.shape}.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"aborted","timestamp":1691690511624,"user":{"displayName":"Ольга Нелюбова","userId":"07415994700883317500"},"user_tz":-480},"id":"j_F6wtYXlwGW"},"outputs":[],"source":["clf = RidgeClassifier()\n","clf.fit(X_train_transform, Y_train)\n","Y_pred = clf.predict(X_test_transform)\n","ROC_AUC1 = roc_auc_score(Y_test, Y_pred)\n","print(f\"ROC-AUC: {ROC_AUC1}.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1691690511624,"user":{"displayName":"Ольга Нелюбова","userId":"07415994700883317500"},"user_tz":-480},"id":"Q710erhglwGW"},"outputs":[],"source":["unique_words = []\n","\n","for item in text:\n","    if item not in unique_words:\n","        unique_words.append(item)\n","        res = ''\n","\n","for parameters in unique_words:\n","    res += ' ' + ' '.join(parameters)\n","\n","q = set(res)\n","\n","# Дополнительные признаки для обучения модели\n","\n","# 1) ИСПОЛЬЗОВАНИЕ ЧРЕЗМЕРНОЙ КАПИТАЛИЗАЦИИ\n","c = sum(bool(X) for X in comments_corpus['comment_text'])\n","caps_lock_train = X_train.apply(lambda x: (len(x.split()))/c)\n","caps_lock_test = X_test.apply(lambda x: (len(x.split()))/c)\n","\n","# 2) ЕСЛИ ВООБЩЕ ВОТ ТАК!!!!!!!!!!!!!!!!!!!!!!!!\n","exclamation_point_train = comments_corpus.loc[X_train.index]['comment_text'].apply(lambda x: x.count('!'))\n","exclamation_point_test = comments_corpus.loc[X_test.index]['comment_text'].apply(lambda x: x.count('!'))\n","\n","# 3) Наркотики\n","censored_drugs = set(['opioids', 'heroin', 'barbiturates', 'krokodil', 'dexamphetamine', 'magic mushrooms', 'LSD', 'cocaine'])\n","censored_drugs_train = comments_corpus.loc[X_train.index]['comment_text'].apply(lambda x:len(set(x.lower().split()).intersection(censored_drugs))/len(set(x.lower().split())))\n","censored_drugs_test = comments_corpus.loc[X_test.index]['comment_text'].apply(lambda x: len(set(x.lower().split()).intersection(censored_drugs))/len(set(x.lower().split())))\n","\n","# 4) Уникальные слова\n","\n","unique_words_train = comments_corpus.loc[X_train.index]['comment_text'].apply(lambda x: len(set(x.lower().split()).intersection(unique_words))/len(set(x.lower().split())))\n","unique_words_test = comments_corpus.loc[X_test.index]['comment_text'].apply(lambda x: len(set(x.lower().split()).intersection(unique_words))/len(set(x.lower().split())))\n","\n","# 5) *и*чт*о*мы*тут*пытаемся*скрыть****звездочками\n","\n","ast_train = comments_corpus.loc[X_train.index]['comment_text'].apply(lambda x: x.count('*'))\n","ast_test = comments_corpus.loc[X_test.index]['comment_text'].apply(lambda x: x.count('*'))\n","\n","# Требуется объединение новых признаков с признаками TF-IDF\n","new_train = csr_matrix(np.array([caps_lock_train, exclamation_point_train, censored_drugs_train, unique_words_train, ast_train]).T)\n","new_test = csr_matrix(np.array([caps_lock_test, exclamation_point_test, censored_drugs_test, unique_words_test, ast_test]).T)\n","\n","# Требуется сложить разреженные матрицы по столбцам\n","X_train_updated= hstack((X_train_transform, new_train))\n","X_test_updated = hstack((X_test_transform, new_test))\n","\n","model = RidgeClassifier()\n","model.fit(X_train_updated, Y_train)\n","Y_pred = model.predict(X_test_updated)\n","ROC_AUC2 = roc_auc_score(Y_test, Y_pred)\n","print(f\"ROC-AUC: {ROC_AUC2} при прошлом значении в {ROC_AUC1}.\")\n","\n","if ROC_AUC2 > ROC_AUC1:\n","    print('Качество классификации улучшилось.')\n","else:\n","    print('Качество классификации ухудшилось.')\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
